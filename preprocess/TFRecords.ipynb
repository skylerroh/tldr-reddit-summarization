{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('/Users/skylerroh/mids/tldr-reddit-summarization')\n",
    "from preprocess import utils\n",
    "import json\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create TFRecords for Model Training and PEGASUS library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000000\n",
      "2000000\n",
      "3000000\n"
     ]
    }
   ],
   "source": [
    "reddit_posts = utils.load_reddit_data_from_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits, content, summary, titles, raw_content = utils.extract_tensors(\n",
    "    [post for post in reddit_posts if 'title' in post and post.get('subreddit') in utils.top50subreddits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dataset = tf.data.Dataset.from_tensor_slices({'topics':subreddits, \n",
    "                                                       'inputs': content, \n",
    "                                                       'targets': summary,\n",
    "                                                       'titles': titles,\n",
    "                                                       'raw_inputs': raw_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tfrecord_file(dataset, filename):\n",
    "    serialized_features_dataset = dataset.map(utils.tf_serialize_example)\n",
    "    writer = tf.data.experimental.TFRecordWriter(filename)\n",
    "    writer.write(serialized_features_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_tfrecord_file(features_dataset.take(1000), 'reddit_small.tfrecord')\n",
    "write_tfrecord_file(features_dataset, 'reddit_all.tfrecord')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = subreddits.shape[0]\n",
    "\n",
    "train_size = int(0.8 * DATASET_SIZE)\n",
    "val_size = int(0.1 * DATASET_SIZE)\n",
    "test_size = int(0.1 * DATASET_SIZE)\n",
    "\n",
    "full_dataset = utils.build('reddit_all.tfrecord', True)\n",
    "full_dataset = full_dataset.shuffle(seed=20200705, buffer_size=2**20)\n",
    "train_dataset = full_dataset.take(train_size)\n",
    "test_dataset = full_dataset.skip(train_size)\n",
    "val_dataset = test_dataset.skip(val_size)\n",
    "test_dataset = test_dataset.take(test_size)\n",
    "\n",
    "datasets = {'train': train_dataset, 'eval': val_dataset, 'test': test_dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, dataset in datasets.items():\n",
    "    filename = f\"reddit_{key}.tfrecord\"\n",
    "    write_tfrecord_file(dataset, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_subreddit(record, subreddit):\n",
    "    return record['topics'] == subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ('test', 'eval', 'train'):\n",
    "    dataset = utils.build(f'reddit_{key}.tfrecord', True)\n",
    "    dataset.cache()\n",
    "    for subreddit in utils.top50subreddits:\n",
    "        sub_r = dataset.filter(lambda record: filter_subreddit(record, subreddit))\n",
    "        filename = f'subreddits_data/subreddit_{subreddit}_{key}.tfrecord'\n",
    "        writer = tf.data.experimental.TFRecordWriter(filename)\n",
    "        write_tfrecord_file(sub_r, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make smaller example sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -- AskReddit\n",
      "1 -- relationships\n",
      "2 -- leagueoflegends\n",
      "3 -- tifu\n",
      "4 -- relationship_advice\n",
      "5 -- trees\n",
      "6 -- gaming\n",
      "7 -- atheism\n",
      "8 -- AdviceAnimals\n",
      "9 -- funny\n",
      "10 -- politics\n",
      "11 -- pics\n",
      "12 -- sex\n",
      "13 -- WTF\n",
      "14 -- explainlikeimfive\n",
      "15 -- todayilearned\n",
      "16 -- Fitness\n",
      "17 -- IAmA\n",
      "18 -- worldnews\n",
      "19 -- DotA2\n",
      "20 -- TwoXChromosomes\n",
      "21 -- videos\n",
      "22 -- DestinyTheGame\n",
      "23 -- reddit.com\n",
      "24 -- offmychest\n",
      "25 -- buildapc\n",
      "26 -- AskMen\n",
      "27 -- personalfinance\n",
      "28 -- summonerschool\n",
      "29 -- technology\n",
      "30 -- wow\n",
      "31 -- NoFap\n",
      "32 -- starcraft\n",
      "33 -- dating_advice\n",
      "34 -- askscience\n",
      "35 -- Games\n",
      "36 -- news\n",
      "37 -- talesfromtechsupport\n",
      "38 -- depression\n",
      "39 -- pcmasterrace\n",
      "40 -- Guildwars2\n",
      "41 -- magicTCG\n",
      "42 -- loseit\n",
      "43 -- GlobalOffensive\n",
      "44 -- electronic_cigarette\n",
      "45 -- movies\n",
      "46 -- self\n",
      "47 -- Advice\n",
      "48 -- Drugs\n",
      "49 -- teenagers\n"
     ]
    }
   ],
   "source": [
    "for i, subreddit in enumerate(utils.top50subreddits):\n",
    "    print(i, \"--\", subreddit)\n",
    "    for key, n in [('train', 2000), ('eval', 100), ('test', 100)]:\n",
    "        dataset = utils.build(f'/Users/skylerroh/mids/tldr-reddit-summarization/preprocess/subreddits_data/subreddit_{subreddit}_{key}.tfrecord', False)\n",
    "        sub_r = dataset.take(n)\n",
    "        filename = f'/Users/skylerroh/mids/tldr-reddit-summarization/subreddits_data/subreddit_{subreddit}_{key}_{n}.tfrecord'\n",
    "        writer = tf.data.experimental.TFRecordWriter(filename)\n",
    "        write_tfrecord_file(sub_r, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, n in [('train', 1000), ('eval', 100)]:\n",
    "    datasets = []\n",
    "    for i, subreddit in enumerate(utils.top50subreddits):\n",
    "        dataset = utils.build(f'/Users/skylerroh/mids/tldr-reddit-summarization/preprocess/subreddits_data/subreddit_{subreddit}_{key}.tfrecord', False)\n",
    "        sub_r = dataset.take(n)\n",
    "        shuffled_data = sub_r.shuffle(n)\n",
    "        datasets.append(shuffled_data)\n",
    "    filename = f'/Users/skylerroh/mids/tldr-reddit-summarization/preprocess/{key}_top50_{n*50//1000}k.tfrecord'\n",
    "    writer = tf.data.experimental.TFRecordWriter(filename)\n",
    "    write_tfrecord_file(tf.data.experimental.sample_from_datasets(datasets), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=utils.build('/Users/skylerroh/mids/tldr-reddit-summarization/preprocess/train_top50_100k.tfrecord', False)\n",
    "for record in dataset:\n",
    "    x=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create extractive summary labels (greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, subreddit in enumerate(utils.top50subreddits):\n",
    "    print(i, \"--\", subreddit)\n",
    "    for key in ['train', 'eval', 'test']:\n",
    "        dataset = utils.build(f'subreddits_data/subreddit_{subreddit}_{key}.tfrecord', False)\n",
    "        sub_r = dataset.take(100)\n",
    "        filename = f'subreddits_data/extractive_subreddit_{subreddit}_{key}_100.tfrecord'\n",
    "        \n",
    "        for example in sub_r:\n",
    "            sent_list = greedy_selection(doc_sent_list, abstract_sent_list, summary_size_sentences, summary_size_unigrams):\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m49",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
